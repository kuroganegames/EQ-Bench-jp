{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjnk7_rqNaGH"
   },
   "source": [
    "# ER-Benchを動かしてみよう\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppCkw--1Ngk1"
   },
   "source": [
    "## EQ-Benchのリポジトリのクローンとセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4224,
     "status": "ok",
     "timestamp": 1734534499984,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "SqaDAfNNHxVU",
    "outputId": "d84b200b-3bad-4612-e5da-9e00b3e3218e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EQ-Bench'...\n",
      "remote: Enumerating objects: 1123, done.\u001b[K\n",
      "remote: Counting objects: 100% (221/221), done.\u001b[K\n",
      "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
      "remote: Total 1123 (delta 180), reused 166 (delta 142), pack-reused 902 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1123/1123), 16.82 MiB | 13.58 MiB/s, done.\n",
      "Resolving deltas: 100% (688/688), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/EQ-bench/EQ-Bench.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50400,
     "status": "ok",
     "timestamp": 1734534550382,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "sjT6Y8zGfJM-",
    "outputId": "0cd71f13-c251-4183-d93b-d0b86cbf8c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# EQ-bench requirements:\n",
    "!pip install -q tqdm sentencepiece hf_transfer openai scipy torch peft bitsandbytes trl accelerate tensorboardX huggingface_hub anthropic scikit-learn matplotlib google-generativeai\n",
    "# These are for qwen models:\n",
    "# !pip install -q einops transformers_stream_generator==0.0.4 deepspeed tiktoken git+https://github.com/Dao-AILab/flash-attention.git auto-gptq optimum\n",
    "# These are for uploading results\n",
    "# !pip install -q gspread oauth2client firebase_admin\n",
    "# Install latest transformers from source last\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3606,
     "status": "ok",
     "timestamp": 1734534553984,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "l-f4UlQ7e7rF",
    "outputId": "527c5e37-b131-4693-f995-ee8346663fc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1734534553985,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "jz1u0WhZLpzc",
    "outputId": "a6a7e73f-aeae-4d63-c2d1-f7ea462d5b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/EQ-Bench\n"
     ]
    }
   ],
   "source": [
    "%cd EQ-Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1734534553985,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "XJSSl60QLuD6",
    "outputId": "5f9ad558-a8bb-477e-9988-aaf86395dd66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.cfg\t\t eq-bench.py\t\tlib\t\t       results\n",
      "data\t\t\t images\t\t\tLICENSE\n",
      "eq_bench_prompts_v1.txt  install_reqs.sh\tooba_quick_install.sh\n",
      "eq_bench_prompts_v2.txt  instruction-templates\tREADME.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL_VNBE5476e"
   },
   "source": [
    "## chat templateの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfBuhhyU8vzi"
   },
   "source": [
    "EQ-Benchはリポジトリ内に独自のchat templateを保存しているため、非対応のテンプレートを用いたモデルの場合はchat templateを作成して保存しておく必要があります。\n",
    "\n",
    "今回使用するllama3は対応していないようなので、chat templateを呼び出して加工・保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1734535571903,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "fW15cc9JKiX9"
   },
   "outputs": [],
   "source": [
    "model_path = \"tokyotech-llm/Llama-3.1-Swallow-8B-v0.2\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1734535572336,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "LDkAtOqJlsql"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1734535572336,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "ZrHvpnI2wQvw"
   },
   "outputs": [],
   "source": [
    "def masked_replace(s_input, s_replace_target, s_replace_to, s_mask_target):\n",
    "  s_mask = \"maskBSSBDfsdbhfabkfb\"\n",
    "  return s_input.replace(s_mask_target, s_mask).replace(s_replace_target, s_replace_to).replace(s_mask, s_mask_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUT1ssWy1pG2"
   },
   "source": [
    "ロールは以下のような置き換えが必要です。\n",
    "\n",
    "- **name_template_user**: `user` -> `<|user|>`\n",
    "- **name_template_assistant**: `assistant` -> `<|bot|>`\n",
    "\n",
    "promptは以下のような置き換えが必要です。\n",
    "\n",
    "- **prompt_template_system**: `system prompt` -> `<|system-message|>`\n",
    "- **prompt_template_user**: `user prompt` -> `<|user-message|>`\n",
    "- **prompt_template_assist**: `assistant response` -> `<|bot-message|>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1734535572336,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "QIhUVudHmCO8"
   },
   "outputs": [],
   "source": [
    "name_template_system = \"system\" #@param {type:\"string\"}\n",
    "name_template_user = \"user\" #@param {type:\"string\"}\n",
    "name_template_assist = \"assistant\" #@param {type:\"string\"}\n",
    "\n",
    "prompt_template_system = \"<|system-message|>\" #@param {type:\"string\"}\n",
    "prompt_template_user = \"<|user-message|>\" #@param {type:\"string\"}\n",
    "prompt_template_assist = \"<|bot-message|>\" #@param {type:\"string\"}\n",
    "\n",
    "chat_dummy = [\n",
    "    {\"role\": name_template_system, \"content\": prompt_template_system},\n",
    "    {\"role\": name_template_user, \"content\": prompt_template_user},\n",
    "    {\"role\": name_template_assist, \"content\": prompt_template_assist},\n",
    "]\n",
    "chat_template = tokenizer.apply_chat_template(chat_dummy, tokenize=False)\n",
    "#print(chat_template)\n",
    "\n",
    "system_dummy = [\n",
    "    {\"role\": \"system\", \"content\": \"<|system-message|>\"},\n",
    "]\n",
    "system_template = tokenizer.apply_chat_template(system_dummy, tokenize=False)\n",
    "\n",
    "chat_template_save = chat_template.replace(\"\\n\",\"\\\\n\").replace(system_template.replace(\"\\n\",\"\\\\n\"), \"\")\n",
    "chat_template_save = masked_replace(chat_template_save, name_template_user, \"<|user|>\", \"<|user-message|>\")\n",
    "chat_template_save = masked_replace(chat_template_save, name_template_assist, \"<|bot|>\", \"<|bot-message|>\")\n",
    "# print(chat_template)\n",
    "\n",
    "# system_template = masked_replace(system_template, name_template_system, \"<|bot|>\", \"<|system-message|>\")\n",
    "system_template_save = system_template.replace(\"\\n\",\"\\\\n\")\n",
    "# print(\"=====\")\n",
    "# print(system_template)\n",
    "\n",
    "\n",
    "name_template_save = \"llama3\" #@param {type:\"string\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnDNR0_J3YQ2"
   },
   "source": [
    "上記の通り置換したうえで、ここで表示されているテンプレートが以下の通りとなっている必要があります。\n",
    "\n",
    "```\n",
    "user: {name_template_user} # テンプレート上のuser名\n",
    "bot: {name_template_assistant} # テンプレート上のLLM側の名前\n",
    "turn_template: # userとassistantだけの文のchat template\n",
    "context: # system templateのみ\n",
    "```\n",
    "\n",
    "tokenizer内のchat templateも参考にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1734535572336,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "ETPpJ7W5mzwJ",
    "outputId": "12fe7405-931f-41fa-f176-25a621eed585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|system-message|><|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|user-message|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|bot-message|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1734535572336,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "_lXfBtAC3XXy",
    "outputId": "4d85e5ef-761f-4aea-8c51-39851fde1002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: \"user\"\n",
      "bot: \"assistant\"\n",
      "turn_template: \"<|start_header_id|><|user|><|end_header_id|>\\n\\n<|user-message|><|eot_id|><|start_header_id|><|bot|><|end_header_id|>\\n\\n<|bot-message|><|eot_id|>\"\n",
      "context: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|system-message|><|eot_id|>\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template_save = f\"user: \\\"{name_template_user}\\\"\\nbot: \\\"{name_template_assist}\\\"\\nturn_template: \\\"{chat_template_save}\\\"\\ncontext: \\\"{system_template_save}\\\"\\n\"\n",
    "# print(\"=====\")\n",
    "print(template_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxp5Div63WNd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1734535572336,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "-veij39a0zQQ"
   },
   "outputs": [],
   "source": [
    "with open(f\"/content/EQ-Bench/instruction-templates/{name_template_save}.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "  f.write(template_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3VzeFcJNliv"
   },
   "source": [
    "## 設定ファイル(config.cfg)の編集\n",
    "\n",
    "次に、EQ-Benchを行うためのモデル設定や評価用のAPI設定などを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1734534595451,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "7s2jbIw4NC5Q"
   },
   "outputs": [],
   "source": [
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1734534595451,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "i4WcUxZtNUFp",
    "outputId": "902f5bc3-0bb8-4b7e-af45-74b92bb48cd4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.cfg']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_ini = configparser.ConfigParser(allow_no_value=True)\n",
    "config_ini.read('config.cfg', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1734534595451,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "XrRGvClyO7Jn"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config_ini' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#@markdown ## [OpenAI]\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#@markdown Optional. Set these if you are testing OpenAI models.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m api_key \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#@param {type:\"string\"}\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mconfig_ini\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#@markdown Optional. Set this if you are using an alternative OpenAI compatible endpoint, like ollama running locally\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#@markdown openai_compatible_url = http://localhost:8080/v1/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#@markdown Optional. This allows the script to download gated models.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m access_token \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#@param {type:\"string\"}\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'config_ini' is not defined"
     ]
    }
   ],
   "source": [
    "#@markdown ## [OpenAI]\n",
    "\n",
    "#@markdown Optional. Set these if you are testing OpenAI models.\n",
    "api_key =  \"\" #@param {type:\"string\"}\n",
    "config_ini[\"OpenAI\"][\"api_key\"] = api_key\n",
    "\n",
    "#@markdown Optional. Set this if you are using an alternative OpenAI compatible endpoint, like ollama running locally\n",
    "\n",
    "#@markdown openai_compatible_url = http://localhost:8080/v1/\n",
    "\n",
    "#@markdown openai_compatible_url = https://api.together.xyz\n",
    "\n",
    "#@markdown ## [Huggingface]\n",
    "\n",
    "#@markdown Optional. This allows the script to download gated models.\n",
    "access_token =  \"\" #@param {type:\"string\"}\n",
    "config_ini[\"Huggingface\"][\"access_token\"] = access_token\n",
    "\n",
    "#@markdown Optional. This is where models will be downloaded to.\n",
    "\n",
    "#@markdown - defaults to ~/.cache/huggingface/hub\n",
    "cache_dir =   \"\" #@param {type:\"string\"}\n",
    "config_ini[\"Huggingface\"][\"cache_dir\"] = cache_dir\n",
    "\n",
    "#@markdown ## [Results upload]\n",
    "\n",
    "#@markdown Optional. Set this to allow uploading of results to a google sheets spreadsheet.\n",
    "\n",
    "#@markdown Note: this feature requires extra configuration (see README).\n",
    "google_spreadsheet_url =  \"\" #@param {type:\"string\"}\n",
    "config_ini[\"Results upload\"][\"google_spreadsheet_url\"] = google_spreadsheet_url\n",
    "\n",
    "#@markdown ## [Creative Writing Benchmark]\n",
    "#@markdown judge_model_api: openai / mistralai / anthropic\n",
    "judge_model_api =  \"anthropic\" #@param [\"openai\", \"mistralai\", \"anthropic\"] {allow-input: true}\n",
    "#@markdown `claude-3-5-sonnet-20241022`が推奨されています。\n",
    "judge_model =  \"claude-3-5-sonnet-20241022\" #@param {type:\"string\"}\n",
    "judge_model_api_key =  \"api_key\" #@param {type:\"string\"}\n",
    "\n",
    "\n",
    "config_ini[\"Creative Writing Benchmark\"][\"judge_model_api\"] = judge_model_api\n",
    "config_ini[\"Creative Writing Benchmark\"][\"judge_model\"] = judge_model\n",
    "config_ini[\"Creative Writing Benchmark\"][\"judge_model_api_key\"] = judge_model_api_key\n",
    "\n",
    "#@markdown ## [Options]\n",
    "\n",
    "#@markdown Set to true or false\n",
    "trust_remote_code = \"true\" #@param [\"true\", \"false\"] {allow-input: true}\n",
    "config_ini[\"Options\"][\"trust_remote_code\"] = trust_remote_code\n",
    "\n",
    "#@markdown ## [Oobabooga config]\n",
    "\n",
    "#@markdown e.g. ~/text-generation-webui/start_linux.sh\n",
    "ooba_launch_script = \"\" #@param {type:\"string\"}\n",
    "config_ini[\"Oobabooga config\"][\"ooba_launch_script\"] = ooba_launch_script\n",
    "\n",
    "#@markdown Specify any additional oobabooga launch params (this can be overridden on a per-model basis).\n",
    "\n",
    "#@markdown e.g.:\n",
    "\n",
    "#@markdown --auto-devices --loader llama.cpp\n",
    "ooba_params_global = \"\" #@param {type:\"string\"}\n",
    "config_ini[\"Oobabooga config\"][\"ooba_params_global\"] = ooba_params_global\n",
    "\n",
    "#@markdown Set to true or false. Setting to \"true\" only supports linux (and possibly mac).\n",
    "\n",
    "#@markdown If set to false, you must launch ooba yourself with --api flag and load the model yourself before running the benchmark.\n",
    "\n",
    "#@markdown If you are launching ooba yourself, the model_path param will be ignored, and all scheduled ooba benchmark runs will use the same model.\n",
    "automatically_launch_ooba = \"false\"  #@param [\"true\", \"false\"] {allow-input: true}\n",
    "config_ini[\"Oobabooga config\"][\"automatically_launch_ooba\"] = automatically_launch_ooba\n",
    "\n",
    "#@markdown Ooba api request timeout in seconds (default 120). Set higher if you are expecting long inference times.\n",
    "ooba_request_timeout = \"600\" #@param {type:\"string\"}\n",
    "config_ini[\"Oobabooga config\"][\"ooba_request_timeout\"] = ooba_request_timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1734534595451,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "BwcwKCFQQ4FP"
   },
   "outputs": [],
   "source": [
    "# 書き込み\n",
    "with open(\"config.cfg\", \"w\", encoding=\"utf-8\") as f: # config.cfgを上書き\n",
    "    config_ini.write(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERWXvWIL8TCK"
   },
   "source": [
    "ベンチマーク用の設定をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1734535646979,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "R7ijS44ZWAU5",
    "outputId": "8ae15e8d-c006-4edd-a743-69454728f79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myrun1,llama3,unsloth/Meta-Llama-3.1-8B-Instruct,,8bit,3,transformers,,\n"
     ]
    }
   ],
   "source": [
    "#@markdown ## [Benchmarks to run]\n",
    "\n",
    "#@markdown **run_id**\n",
    "#@markdown   - ユーザーが任意で指定する、ベンチマーク実行を識別するための名前。\n",
    "#@markdown   - 出力ファイルやログ内で、このIDを用いて結果を区別します。\n",
    "run_id = \"myrun1\" #@param {type:\"string\"}\n",
    "# if run_id:\n",
    "#   config_ini[\"Benchmarks to run\"][\"run_id\"] = run_id\n",
    "\n",
    "#@markdown **instruction_template**\n",
    "#@markdown   - プロンプト形式を定義するテンプレート名（`.yaml`拡張子なし）\n",
    "#@markdown   - `instruction-templates`ディレクトリ内にある対応する`.yaml`ファイルに準拠したフォーマットでLLMへの指示（プロンプト）を生成します。\n",
    "instruction_template = \"\" #@param {type:\"string\"}\n",
    "instruction_template = instruction_template if instruction_template != \"\" else name_template_save\n",
    "\n",
    "# if instruction_template:\n",
    "#   config_ini[\"Benchmarks to run\"][\"instruction_template\"] = instruction_template\n",
    "\n",
    "#@markdown **model_path**\n",
    "#@markdown   - 使用するモデルを指定するパスまたはモデルID。\n",
    "#@markdown   - Hugging FaceのモデルID（例：`meta-llama/Llama-2-7b-chat-hf`）\n",
    "#@markdown   - ローカルファイルパス（例：`~/my_local_model`）\n",
    "#@markdown   - OpenAIや他のAPI互換モデルの場合は対応するモデル名（例：`gpt-4-0613`）\n",
    "#@markdown   - inference_engineによって解釈が異なります。\n",
    "#@markdown   - **model_path**は最初のmodel_pathを設定します。\n",
    "# model_path = \"unsloth/Meta-Llama-3.1-8B-Instruct\" #@param {type:\"string\"}\n",
    "# if model_path:\n",
    "#   config_ini[\"Benchmarks to run\"][\"model_path\"] = model_path\n",
    "\n",
    "#@markdown **lora_path**（オプション）\n",
    "#@markdown   - LoRAアダプタを使用する場合、そのアダプタファイルまたはディレクトリへのパス。\n",
    "#@markdown   - 指定しない場合は空欄で可。\n",
    "lora_path = \"\" #@param {type:\"string\"}\n",
    "# if lora_path:\n",
    "#   config_ini[\"Benchmarks to run\"][\"lora_path\"] = lora_path\n",
    "\n",
    "#@markdown **quantization**\n",
    "#@markdown   - モデルロード時の量子化設定。\n",
    "#@markdown   - `8bit`, `4bit`, `None` (または空)が有効。\n",
    "#@markdown   - BitsAndBytesによるGPUメモリ削減のために使用します。\n",
    "quantization = \"8bit\"  #@param [\"None\", \"4bit\", \"8bit\"] {allow-input: true}\n",
    "# if quantization:\n",
    "#   config_ini[\"Benchmarks to run\"][\"quantization\"] = quantization\n",
    "\n",
    "#@markdown **n_iterations**\n",
    "#@markdown   - ベンチマークを何回繰り返すかを指定する整数値。\n",
    "#@markdown   - 出力スコアは、この回数分の実行結果を平均したものが用いられます。\n",
    "#@markdown   - 例：`3` とすれば3回実行して平均スコアを求めます。\n",
    "n_iterations = \"3\" #@param {type:\"string\"}\n",
    "# if n_iterations:\n",
    "#   config_ini[\"Benchmarks to run\"][\"n_iterations\"] = n_iterations\n",
    "\n",
    "#@markdown **inference_engine**\n",
    "#@markdown   - 推論エンジンを指定する文字列。\n",
    "#@markdown   - 例：`transformers`, `ooba`, `openai`, `llama.cpp`, `anthropic`, `mistralai`, `gemini`など\n",
    "#@markdown   - モデルの実行方法（ローカル/外部APIなど）を決定します。\n",
    "inference_engine = \"transformers\" #@param {type:\"string\"}\n",
    "# if inference_engine:\n",
    "#   config_ini[\"Benchmarks to run\"][\"inference_engine\"] = inference_engine\n",
    "\n",
    "#@markdown **ooba_params**（オプション）\n",
    "#@markdown   - inference_engineが`ooba`の場合のみ有効で、モデルロード時や推論時の追加パラメータを指定します。\n",
    "#@markdown   - `--loader transformers --n_ctx 1024 --n-gpu-layers -1` といった形式で、oobaboogaのWebUI起動パラメータをカンマ区切りで記述します。\n",
    "ooba_params = \"\" #@param {type:\"string\"}\n",
    "# if ooba_params:\n",
    "#   config_ini[\"Benchmarks to run\"][\"ooba_params\"] = ooba_params\n",
    "\n",
    "#@markdown **downloader_filters**（オプション）\n",
    "#@markdown   - Hugging Faceからモデルをダウンロードする際のフィルタ設定。\n",
    "#@markdown   - `--include`や`--exclude`パターンを使用し、特定のファイルのみダウンロードしたり除外したりできます。\n",
    "#@markdown   - 例：`--include [\"*Q3_K_M.gguf\", \"*.json\"]`\n",
    "#@markdown   - 記載しない場合は空欄で可。\n",
    "downloader_args = \"\" #@param {type:\"string\"}\n",
    "# if downloader_args:\n",
    "#   config_ini[\"Benchmarks to run\"][\"downloader_args\"] = downloader_args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "setting_bench = \",\".join([run_id,instruction_template,model_path,lora_path,quantization,n_iterations,inference_engine,ooba_params,downloader_args])\n",
    "print(setting_bench)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbWeVc_w8IjC"
   },
   "source": [
    "`config.cfg`にベンチマーク用の設定`setting_bench`を書き込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1734535661700,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "MQXuHTKM7oFM"
   },
   "outputs": [],
   "source": [
    "with open(\"config.cfg\", \"r\", encoding=\"utf-8\") as reader:\n",
    "  config_txt = reader.read()\n",
    "\n",
    "section_benchmark_setting = \"[Benchmarks to run]\"\n",
    "config_txt = config_txt.replace(section_benchmark_setting, section_benchmark_setting+\"\\n\"+setting_bench)\n",
    "\n",
    "with open(\"config.cfg\", \"w\", encoding=\"utf-8\") as writer:\n",
    "  writer.write(config_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Hjeia3W5BVg"
   },
   "source": [
    "## ベンチマーク実行部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6IbUqLhQ4SE",
    "outputId": "2cd0698a-28bf-4f5f-f025-6b7043b0b7dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-18 15:29:47.099316: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-18 15:29:47.118718: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-18 15:29:47.124487: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-18 15:29:47.138388: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-18 15:29:48.433841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "--------------\n",
      "Running benchmark 1 of 1\n",
      "\n",
      "unsloth/Meta-Llama-3.1-8B-Instruct\n",
      "--------------\n",
      "Iteration 1 of 3\n",
      "Loading checkpoint shards: 100% 4/4 [00:58<00:00, 14.63s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "  0% 0/24 [00:00<?, ?it/s]Device set to use cuda:0\n",
      "All writes completed. Exiting gracefully.\n",
      "  0% 0/24 [00:54<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!python eq-bench.py --benchmarks creative-writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1734521300772,
     "user": {
      "displayName": "火巻鉄",
      "userId": "03989691914334229796"
     },
     "user_tz": -540
    },
    "id": "0H6cxX4odLba"
   },
   "outputs": [],
   "source": [
    "!python eq-bench.py --benchmarks eq-bench"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOJ/EBJFWH6jQdsWZoD3YcI",
   "collapsed_sections": [
    "OL_VNBE5476e"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
